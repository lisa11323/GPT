{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisa11323/GPT/blob/main/GPT_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing"
      ],
      "metadata": {
        "id": "JJhu_GKIm698"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Preprocessing Code for Zero-Shot Classification"
      ],
      "metadata": {
        "id": "xfuWypgynIOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAw-BCPPm4Cj"
      },
      "outputs": [],
      "source": [
        "# Remove non-English characters and emojis, preprocess syllables, then apply filtering\n",
        "\n",
        "!pip install -U pandas numpy\n",
        "!pip install nltk emoji\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    text = emoji.replace_emoji(text, replace=\"\")\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df['Review'] = df['Review'].progress_apply(clean_text)\n",
        "\n",
        "def get_review_length(text):\n",
        "    try:\n",
        "        return len(str(text).split())\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "df['Review_length'] = df['Review'].progress_apply(get_review_length)\n",
        "\n",
        "filter_by_length = True\n",
        "min_length = 10            # standard length\n",
        "\n",
        "if filter_by_length:\n",
        "    df = df[df['Review_length'] >= min_length].reset_index(drop=True)\n",
        "\n",
        "preprocessed_file_path = 'preprocessed_comments_with_length.xlsx'\n",
        "df.to_excel(preprocessed_file_path, index=False)\n",
        "print(f\"Saved preprocessed and review length filtered ({filter_by_length}) data to '{preprocessed_file_path}'\")\n",
        "\n",
        "files.download(preprocessed_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Preprocessing code for BERTopic"
      ],
      "metadata": {
        "id": "vMk7wWI0nMV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install nltk\n",
        "!pip install sentence-transformers\n",
        "!pip install hdbscan\n",
        "!pip install plotly\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    file_path = fn\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "print(f\"File loaded: {file_path}\")\n",
        "\n",
        "# Preparing for stopword removal and lemmatization\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define preprocessing function (modified to retain English only)\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in default_stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "if \"Review\" in df.columns:\n",
        "    tqdm.pandas()\n",
        "    df = df.rename(columns={\"Review\": \"Review_Raw\"})\n",
        "    df[\"Review_preprocessed\"] = df[\"Review_Raw\"].progress_apply(preprocess_text)\n",
        "    print(\"Preprocessing complete\")\n",
        "else:\n",
        "    print(\"The 'Review' column does not exist.\")\n",
        "\n",
        "output_file = \"preprocessed_Review.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "8-Z5n-yxnQDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Topic modeling using BERTopic with Louvain clustering"
      ],
      "metadata": {
        "id": "XLwtbsYroO9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Install required packages"
      ],
      "metadata": {
        "id": "yxehKv5KpKne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl bertopic tqdm hdbscan plotly umap-learn matplotlib python-louvain"
      ],
      "metadata": {
        "id": "XEgT-kCEoUAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Load BERTopic model"
      ],
      "metadata": {
        "id": "MsBRcPFhpKL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from google.colab import files\n",
        "import hdbscan\n",
        "from umap import UMAP\n",
        "import tensorflow_hub as hub\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    file_path = fn\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "print(f\"File loaded: {file_path}\")\n",
        "print(\"Preview the 'Review_preprocessed' column:\")\n",
        "print(df[\"Review_preprocessed\"].head())\n",
        "\n",
        "# Define text preprocessing function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "comments = df[\"Review_preprocessed\"].dropna().astype(str).tolist()\n",
        "comments_cleaned = [clean_text(text) for text in tqdm(comments)]\n",
        "\n",
        "# Configure hDBSCAN clustering\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=50,\n",
        "    min_samples=5,\n",
        "    metric='euclidean',\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic(hdbscan_model=hdbscan_model, language=\"english\", verbose=True)\n",
        "topics, probs = topic_model.fit_transform(comments_cleaned)\n",
        "\n",
        "# Visualize\n",
        "fig = topic_model.visualize_topics()\n",
        "fig.show()\n",
        "\n",
        "df_result = df.copy()\n",
        "df_result[\"topic_id\"] = topics\n",
        "\n",
        "output_file = \"bertopic_result.xlsx\"\n",
        "df_result.to_excel(output_file, index=False)\n",
        "\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "zOjpuWzJof8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract topic information"
      ],
      "metadata": {
        "id": "tprdwo4ypJWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Retrieve topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.colheader_justify\", \"center\")\n",
        "\n",
        "# Display as a table in Colab\n",
        "from IPython.display import display\n",
        "display(topic_info)\n",
        "\n",
        "output_path = \"topic_info.xlsx\"\n",
        "topic_info.to_excel(output_path, index=False)\n",
        "print(f\"Saved topic information: {output_path}\")\n",
        "\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "bEZ-MWYTpD6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Extract representative keywords for each topic"
      ],
      "metadata": {
        "id": "a1sonO_KpUa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Representative keywords for each topic\n",
        "print(\"Representative keywords for each topic:\")\n",
        "for topic_id in topic_model.get_topics().keys():\n",
        "    if topic_id == -1:\n",
        "        continue\n",
        "    print(f\"\\nTopic {topic_id}:\")\n",
        "    for word, weight in topic_model.get_topic(topic_id)[:10]:\n",
        "        print(f\"  - {word} ({weight:.4f})\")\n",
        "\n",
        "# Compute similarity matrix using topic embeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "embeddings = topic_model.topic_embeddings_\n",
        "\n",
        "similarity_matrix = cosine_similarity(embeddings)"
      ],
      "metadata": {
        "id": "8npR-rsYpT6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 Visualize sentence clusters"
      ],
      "metadata": {
        "id": "KENbC1JzpcnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embeddings = embedding_model.encode(comments_cleaned, show_progress_bar=True)\n",
        "\n",
        "# 3. Reduce dimensions using UMAP\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "umap_model = UMAP(n_components=2, random_state=42)\n",
        "embeddings_2d = umap_model.fit_transform(embeddings)\n",
        "\n",
        "# Visualize\n",
        "unique_topics = sorted(set(topics))\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_topics))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for topic in unique_topics:\n",
        "    idx = [i for i, t in enumerate(topics) if t == topic]\n",
        "    plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1],\n",
        "                label=f\"Topic {topic}\", alpha=0.5, s=30, color=colors(topic))\n",
        "\n",
        "plt.title(\"Visualize sentence clusters (UMAP + Topic Color)\")\n",
        "plt.xlabel(\"UMAP 1\")\n",
        "plt.ylabel(\"UMAP 2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYywmHhNpjw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5 Visualize hierarchical_topics"
      ],
      "metadata": {
        "id": "EmnzrYzjpudx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig_hierarchy = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
        "\n",
        "fig_hierarchy.show()"
      ],
      "metadata": {
        "id": "-ur08t3-pmeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.6 Visualize BERTopic"
      ],
      "metadata": {
        "id": "z2L9tqgApyzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig_topics = topic_model.visualize_topics()\n",
        "\n",
        "fig_topics.show()"
      ],
      "metadata": {
        "id": "ScMTL0DQp_AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.7 Visualize Barchart"
      ],
      "metadata": {
        "id": "e987p5sPqAf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig_barchart = topic_model.visualize_barchart(top_n_topics=137)\n",
        "\n",
        "fig_barchart.show()"
      ],
      "metadata": {
        "id": "15j3d1ELqCrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.8 Visualize heatmap"
      ],
      "metadata": {
        "id": "F1sPuAXbq1I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig_heatmap = topic_model.visualize_heatmap()\n",
        "\n",
        "fig_heatmap.show()"
      ],
      "metadata": {
        "id": "8n18rCNwqG6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.9 Load Louvain algorithm model"
      ],
      "metadata": {
        "id": "ZwZT0AdyqJWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import community.community_louvain as community_louvain\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute topic-wise average embedding vectors\n",
        "topic_ids = sorted(set(topics))\n",
        "topic_vectors = []\n",
        "\n",
        "for tid in tqdm(topic_ids, desc=\"Embedding topics\"):\n",
        "    idx = [i for i, t in enumerate(topics) if t == tid]\n",
        "    topic_sentences = [comments_cleaned[i] for i in idx]\n",
        "    if topic_sentences:\n",
        "        topic_embedding = embedding_model.encode(topic_sentences)\n",
        "        topic_mean = np.mean(topic_embedding, axis=0)\n",
        "        topic_vectors.append(topic_mean)\n",
        "    else:\n",
        "        topic_vectors.append(np.zeros(embedding_model.get_sentence_embedding_dimension()))\n",
        "\n",
        "# 2. Compute similarity matrix\n",
        "similarity_matrix = cosine_similarity(topic_vectors)\n",
        "\n",
        "G = nx.Graph()\n",
        "for i in tqdm(range(len(topic_ids)), desc=\"Generating graph edges\"):\n",
        "    for j in range(i + 1, len(topic_ids)):\n",
        "        weight = similarity_matrix[i][j]\n",
        "\n",
        "        if weight > 0.7:\n",
        "          G.add_edge(topic_ids[i], topic_ids[j], weight=weight)\n",
        "\n",
        "# Louvain community detection\n",
        "partition = community_louvain.best_partition(G)\n",
        "\n",
        "# Assign isolated topics to meta-topics\n",
        "isolated_topics = list(set(topic_ids) - set(partition.keys()))\n",
        "if isolated_topics:\n",
        "    next_meta_topic = max(partition.values()) + 1 if partition else 0\n",
        "    for iso_tid in isolated_topics:\n",
        "        partition[iso_tid] = next_meta_topic\n",
        "        next_meta_topic += 1\n",
        "\n",
        "meta_topic_dict = {}\n",
        "for topic_id, community_id in partition.items():\n",
        "    if community_id not in meta_topic_dict:\n",
        "        meta_topic_dict[community_id] = []\n",
        "    meta_topic_dict[community_id].append(topic_id)\n",
        "\n",
        "# Result\n",
        "for group, topics_in_group in meta_topic_dict.items():\n",
        "    print(f\"\\n Meta-topic {group}:\")\n",
        "    print(f\"Topic IDs: {topics_in_group}\")\n",
        "    for tid in topics_in_group:\n",
        "        words = topic_model.get_topic(tid)[:5]\n",
        "        keywords = \", \".join([w[0] for w in words])\n",
        "        print(f\"  - Topic {tid}: {keywords}\")"
      ],
      "metadata": {
        "id": "-OPojmy2qM3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_with_topic = df.copy()\n",
        "df_with_topic[\"topic_id\"] = topics\n",
        "\n",
        "# Mapping from topic_id to meta_topic_id\n",
        "topic_to_meta = partition  # Results from Louvain clustering\n",
        "\n",
        "df_with_topic[\"meta_topic_id\"] = df_with_topic[\"topic_id\"].map(topic_to_meta)\n",
        "\n",
        "# Exclude -1 labels considered as noise\n",
        "df_with_topic = df_with_topic[df_with_topic[\"topic_id\"] != -1]\n",
        "\n",
        "df_with_topic = df_with_topic.sort_values(by=[\"meta_topic_id\", \"topic_id\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"Review data grouped by meta-topic:\")\n",
        "print(df_with_topic[[\"meta_topic_id\", \"topic_id\", \"Review_preprocessed\"]].head())\n",
        "\n",
        "output_file = \"meta_topic_grouped_reviews.xlsx\"\n",
        "df_with_topic.to_excel(output_file, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "wChBbJcOqO2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table of topic–meta-topic–keywords (+frequency)\n",
        "rows = []\n",
        "\n",
        "for topic_id, meta_topic in partition.items():\n",
        "    if topic_id == -1:\n",
        "        continue\n",
        "\n",
        "    keywords = topic_model.get_topic(topic_id)\n",
        "\n",
        "    for word, weight in keywords[:10]:\n",
        "        rows.append({\n",
        "            \"meta_topic_id\": meta_topic,\n",
        "            \"topic_id\": topic_id,\n",
        "            \"keyword\": word,\n",
        "            \"score\": round(weight, 4)\n",
        "        })\n",
        "\n",
        "df_keywords = pd.DataFrame(rows)\n",
        "\n",
        "df_keywords = df_keywords.sort_values(by=[\"meta_topic_id\", \"topic_id\", \"score\"], ascending=[True, True, False]).reset_index(drop=True)\n",
        "\n",
        "output_keywords_file = \"meta_topic_keywords_with_scores.xlsx\"\n",
        "df_keywords.to_excel(output_keywords_file, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_keywords_file)"
      ],
      "metadata": {
        "id": "B_-JovqtqQlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Code for measuring independent variables"
      ],
      "metadata": {
        "id": "DilHOz_iq3_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install scipy transformers pandas openpyxl tqdm matplotlib datasets scikit-learn --quiet"
      ],
      "metadata": {
        "id": "t5eZJUr5r6dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 App experience"
      ],
      "metadata": {
        "id": "l2xouDggrXEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'Review' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for App Experience\n",
        "app_experience_labels = [\n",
        "    (\"Completely Unrelated\", 0.0),\n",
        "    (\"Slightly Unrelated\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Related\", 0.75),\n",
        "    (\"Strongly Related\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_app_experience_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in app_experience_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment is {} to the app experience.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output[0]\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, app_experience_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"AppExperienceRelevance_score\"] = df[\"Review_Raw\"].progress_apply(classify_app_experience_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_app_experience_relevance_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: AppExperienceRelevance_score (0~1).\")"
      ],
      "metadata": {
        "id": "zLaYuU2VrYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Chatbot interaction"
      ],
      "metadata": {
        "id": "MPB-luI4rcd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'Review' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Chatbot Interaction\n",
        "chatbot_labels = [\n",
        "    (\"Completely Unrelated\", 0.0),\n",
        "    (\"Slightly Unrelated\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Related\", 0.75),\n",
        "    (\"Strongly Related\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_chatbot_interaction_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in chatbot_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment is {} to the chatbot interaction.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output[0]\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, chatbot_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"ChatbotInteractionRelevance_score\"] = df[\"Review_Raw\"].progress_apply(classify_chatbot_interaction_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_chatbot_interaction_relevance_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: ChatbotInteractionRelevance_score (0~1).\")"
      ],
      "metadata": {
        "id": "Dd5CFfeoroAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Learning support"
      ],
      "metadata": {
        "id": "ME_mhH2brc8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'Review' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Learning Support\n",
        "learning_support_labels = [\n",
        "    (\"Completely Unrelated\", 0.0),\n",
        "    (\"Slightly Unrelated\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Related\", 0.75),\n",
        "    (\"Strongly Related\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_learning_support_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in learning_support_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment is {} to the learning support.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output[0]\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, learning_support_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"LearningSupportRelevance_score\"] = df[\"Review_Raw\"].progress_apply(classify_learning_support_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_learning_support_relevance_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: LearningSupportRelevance_score (0~1).\")"
      ],
      "metadata": {
        "id": "rl-Du9MLroUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 Feature request"
      ],
      "metadata": {
        "id": "qU2UEn2PrdHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'Review' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Feature Request\n",
        "feature_request_labels = [\n",
        "    (\"Completely Unrelated\", 0.0),\n",
        "    (\"Slightly Unrelated\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Related\", 0.75),\n",
        "    (\"Strongly Related\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_feature_request_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in feature_request_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment is {} to the feature request.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output[0]\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, feature_request_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"FeatureRequestRelevance_score\"] = df[\"Review_Raw\"].progress_apply(classify_feature_request_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_feature_request_relevance_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: FeatureRequestRelevance_score (0~1).\")"
      ],
      "metadata": {
        "id": "MUQS_KRArok3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5 Bias concerns"
      ],
      "metadata": {
        "id": "ydl-YcqmrdRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'Review' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Bias Concerns\n",
        "bias_concerns_labels = [\n",
        "    (\"Completely Unrelated\", 0.0),\n",
        "    (\"Slightly Unrelated\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Related\", 0.75),\n",
        "    (\"Strongly Related\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_bias_concerns_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in bias_concerns_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment is {} to the bias concerns.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output[0]\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, bias_concerns_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"BiasConcernsRelevance_score\"] = df[\"Review_Raw\"].progress_apply(classify_bias_concerns_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_bias_concerns_relevance_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: BiasConcernsRelevance_score (0~1).\")"
      ],
      "metadata": {
        "id": "5_k0M4efq3aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Code for measuring moderator"
      ],
      "metadata": {
        "id": "KvfdQHuGruWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Human-likeness"
      ],
      "metadata": {
        "id": "BOFSWCw1sJ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'comment' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Humanlikeness\n",
        "humanlikeness_labels = [\n",
        "    (\"Very Machianlike\", 0.0),\n",
        "    (\"Somewhat Machianlike\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Humanlike\", 0.75),\n",
        "    (\"Very Humanlike\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_humanlikeness_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in humanlikeness_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment mention to {} in terms of uncanny valley theory in conversational AI.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, humanlikeness_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"Humanlikeness_score\"] = df[\"Review_Raw\"].progress_apply(classify_humanlikeness_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_humanlikeness_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: Humanlikeness_score (0~1).\")"
      ],
      "metadata": {
        "id": "8y7nFscVry2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Familiarity"
      ],
      "metadata": {
        "id": "K4n_blPvsOwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'comment' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Familiarity 라벨 정의\n",
        "familiarity_labels = [\n",
        "    (\"Very Unfamiliar\", 0.0),\n",
        "    (\"Somewhat Unfamiliar\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Familiar\", 0.75),\n",
        "    (\"Very Familiar\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_familiarity_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in familiarity_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This comment mention to {} in terms of uncanny valley theory in conversational AI.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, familiarity_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"Familiarity_score\"] = df[\"Review_Raw\"].progress_apply(classify_familiarity_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_familiarity_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: Familiarity_score (0~1).\")"
      ],
      "metadata": {
        "id": "UcHHK5BVsYD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Code for measuring mediator"
      ],
      "metadata": {
        "id": "KFLbFXHqsZo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1 Subjectivity"
      ],
      "metadata": {
        "id": "JCc4GnkUsfbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'comment' column does not exist.\")\n",
        "\n",
        "# Load the zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define relevance labels for Subjectivity\n",
        "subjectivity_labels = [\n",
        "    (\"Very Objective\", 0.0),\n",
        "    (\"Somewhat Objective\", 0.25),\n",
        "    (\"Neutral\", 0.5),\n",
        "    (\"Somewhat Subjective\", 0.75),\n",
        "    (\"Very Subjective\", 1.0)\n",
        "]\n",
        "\n",
        "def classify_subjectivity_score(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "\n",
        "    labels = [label for label, _ in subjectivity_labels]\n",
        "\n",
        "    output = classifier(\n",
        "        comment,\n",
        "        candidate_labels=labels,\n",
        "        hypothesis_template=\"This sentence expresses a {} opinion.\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    if isinstance(output, list):\n",
        "        result = output\n",
        "    elif isinstance(output, dict) and \"scores\" in output:\n",
        "        result = [\n",
        "            {'label': label, 'score': score}\n",
        "            for label, score in zip(output[\"labels\"], output[\"scores\"])\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output format.\")\n",
        "\n",
        "    weighted = sum(r['score'] * w for r, (_, w) in zip(result, subjectivity_labels))\n",
        "    return round(weighted, 4)\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"Subjectivity_score\"] = df[\"Review_Raw\"].progress_apply(classify_subjectivity_score)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_subjectivity_scored.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"Finished: Subjectivity_score (0~1).\")"
      ],
      "metadata": {
        "id": "AV4ocDXSsesj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Code for measuring dependent variables"
      ],
      "metadata": {
        "id": "3kbcbJlOst0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1 Code for sentiment analysis based on RoBERTa model"
      ],
      "metadata": {
        "id": "m3F-9vjis8CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_excel(filename)\n",
        "if 'Review_Raw' not in df.columns:\n",
        "    raise ValueError(\"The 'Review' column does not exist.\")\n",
        "df['Review_Raw'] = df['Review_Raw'].fillna(\"\")\n",
        "\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
        "\n",
        "# Sentiment analysis function (batch mode)\n",
        "def analyze_sentiment_batch(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    probs = softmax(outputs.logits.cpu().numpy(), axis=1)\n",
        "    return probs  # shape: (batch_size, 3)\n",
        "\n",
        "batch_size = 32\n",
        "scores = []\n",
        "\n",
        "for i in tqdm(range(0, len(df), batch_size)):\n",
        "    batch_texts = df['Review_Raw'].iloc[i:i+batch_size].tolist()\n",
        "    try:\n",
        "        probs = analyze_sentiment_batch(batch_texts)\n",
        "        for prob in probs:\n",
        "            pos, neu, neg = prob[2], prob[1], prob[0]\n",
        "            compound = round((pos - neg) * (1 - neu), 4)\n",
        "            scores.append(compound)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occured at batch {i}: {e}\")\n",
        "        scores.extend([0.0] * len(batch_texts))\n",
        "\n",
        "df['sentiment_score'] = scores\n",
        "\n",
        "# Compute the 25th and 75th percentile values\n",
        "q25 = df['sentiment_score'].quantile(0.25)\n",
        "q75 = df['sentiment_score'].quantile(0.75)\n",
        "\n",
        "print(f\"Quantile cutoff values — Q25: {q25:.4f}, Q75: {q75:.4f}\")\n",
        "\n",
        "def label_sentiment(score):\n",
        "    if score <= q25:\n",
        "        return 2  # Negative\n",
        "    elif score >= q75:\n",
        "        return 1  # Positive\n",
        "    else:\n",
        "        return 3  # Neutral\n",
        "\n",
        "df['sentiment_label'] = df['sentiment_score'].apply(label_sentiment)\n",
        "\n",
        "output_file = filename.replace(\".xlsx\", \"_roberta_sentiment_labeled.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(f\"Finished: RoBERTa sentiment analysis and quantile labeling completed (1 = Positive ≥ {q75:.2f}, 2 = Negative ≤ {q25:.2f}, 3 = Neutral)\")"
      ],
      "metadata": {
        "id": "-WGRt54WtBx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}